openapi: 3.0.0
info:
  title: OpenAI API
  version: "1.2.0"
  x-ballerina-display:
    label: OpenAI Chat
    iconPath: "icon.png"
  description: >-
    This is the [OpenAI API] (https://platform.openai.com/docs/api-reference/introduction) specification. Use the OpenAI API to access the state-of-the-art language models that can complete sentences, transcribe audio, and generate images. The API also supports natural language processing tasks such as text classification, entity recognition, and sentiment analysis. By using the OpenAI API, you can incorporate advanced AI capabilities into your own applications and services.
  x-ballerina-init-description: >-
    To use the OpenAI API, you will need an API key. You can sign up for an API key by creating an [account](https://beta.openai.com/signup/) on the OpenAI website and following the [provided instructions](https://platform.openai.com/docs/api-reference/authentication).
servers:
  - url: https://api.openai.com/v1
tags:
  - name: OpenAI
    description: The OpenAI REST API
security:
  - BearerAuth: []
paths:
 /chat/completions:
    post:
      x-ballerina-display:
        label: Create Chat Completion
      operationId: createChatCompletion
      tags:
        - OpenAI
        - chat
      summary: Creates a completion for the chat message
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateChatCompletionRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"

      x-oaiMeta:
        name: Create chat completion
        group: chat
        path: create
        beta: true
        examples:
          curl: |
            curl https://api.openai.com/v1/chat/completions \
              -H 'Content-Type: application/json' \
              -H 'Authorization: Bearer YOUR_API_KEY' \
              -d '{
              "model": "gpt-3.5-turbo",
              "messages": [{"role": "user", "content": "Hello!"}]
            }'
          python: |
            import os
            import openai
            openai.api_key = os.getenv("OPENAI_API_KEY")

            completion = openai.ChatCompletion.create(
              model="gpt-3.5-turbo",
              messages=[
                {"role": "user", "content": "Hello!"}
              ]
            )

            print(completion.choices[0].message)
          node.js: |
            const { Configuration, OpenAIApi } = require("openai");

            const configuration = new Configuration({
              apiKey: process.env.OPENAI_API_KEY,
            });
            const openai = new OpenAIApi(configuration);

            const completion = await openai.createChatCompletion({
              model: "gpt-3.5-turbo",
              messages: [{role: "user", content: "Hello world"}],
            });
            console.log(completion.data.choices[0].message);
        parameters: |
          {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": "Hello!"}]
          }
        response: |
          {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "choices": [{
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "\n\nHello there, how may I assist you today?",
              },
              "finish_reason": "stop"
            }],
            "usage": {
              "prompt_tokens": 9,
              "completion_tokens": 12,
              "total_tokens": 21
            }
          }

components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
  schemas:
    ChatCompletionRequestMessage:
      type: object
      properties:
        role:
          type: string
          enum: ["system", "user", "assistant"]
          description: The role of the author of this message.
        content:
          type: string
          description: The contents of the message
        name:
          type: string
          description: The name of the user in a multi-user chat
      required:
        - role
        - content

    ChatCompletionResponseMessage:
      type: object
      properties:
        role:
          type: string
          enum: ["system", "user", "assistant"]
          description: The role of the author of this message.
        content:
          type: string
          description: The contents of the message
      required:
        - role
        - content

    CreateChatCompletionRequest:
      type: object
      properties:
        model:
          description: ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.
          type: string
        messages:
          description: The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).
          type: array
          minItems: 1
          items:
            $ref: "#/components/schemas/ChatCompletionRequestMessage"
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: |
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: |
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: How many chat completion choices to generate for each input message.
        stream:
          description: >
            If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message.
          type: boolean
          nullable: true
          default: false
        stop:
          description: |
            Up to 4 sequences where the API will stop generating further tokens.

          oneOf:
            - type: string
              nullable: true
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
        max_tokens:
          description: |
            The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).
          default: inf
          type: integer
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

            [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
        logit_bias:
          type: object
          x-oaiTypeLabel: map


          description: |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        user:           
          type: string
          example: user-1234
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).

      required:
        - model
        - messages

    CreateChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            type: object
            properties:
              index:
                type: integer
              message:
                $ref: "#/components/schemas/ChatCompletionResponseMessage"
              finish_reason:
                type: string
        usage:
          type: object
          properties:
            prompt_tokens:
              type: integer
            completion_tokens:
              type: integer
            total_tokens:
              type: integer
          required:
            - prompt_tokens
            - completion_tokens
            - total_tokens
      required:
        - id
        - object
        - created
        - model
        - choices
x-oaiMeta:
  groups:
    - id: completions
      title: Completions
      description: |
        Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
    - id: chat
      title: Chat
      description: |
        Given a chat conversation, the model will return a chat completion response.
